---
title: Troubleshooting für Azure Data Factory | Microsoft-Dokumentation
description: Troubleshooting für Azure Data Factory. Allgemeines Dokument für alle externen Steuerungsaktivitäten.
services: data-factory
author: abnarain
manager: craigg
ms.service: data-factory
ms.topic: troubleshoot
ms.subservice: troubleshoot
ms.date: 6/26/2019
ms.author: abnarain
ms.reviewer: craigg
ms.openlocfilehash: 8d6ab565098e1ea40ede5c650f05e670a1edc7f6
ms.sourcegitcommit: f56b267b11f23ac8f6284bb662b38c7a8336e99b
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 06/28/2019
ms.locfileid: "67454237"
---
# <a name="troubleshooting-azure-data-factory"></a>Problembehandlung für Azure Data Factory
Dieser Artikel enthält allgemeine Fragen zur Problembehandlung.

- [Azure Databricks (Notebook, JARs, Python)](#azure-databricks)
- [Azure Data Lake Analytics (U-SQL)](#azure-data-lake-analytics-u-sql)
- [Azure-Funktionen](#azure-functions)
- [Benutzerdefiniert (Azure Batch)](#custom-azure-batch)
- [HDInsight (Spark, Hive, MapReduce, Pig, Hadoop Streaming)](#hdinsight-spark-hive-mapreduce-pig-hadoop-streaming)
- [Webaktivität](#web-activity)



## <a name="azure-databricks"></a>Azure Databricks
| Fehlercode | Fehlermeldung                                          | Problembeschreibung                             | Mögliche Behebung / Empfohlene Aktion                            |
| -------------- | ----------------------------------------------------- | --------------------------------------------------------------| :----------------------------------------------------------- |
| 3200           | Fehler 403                                                    | Databricks-Zugriffstoken ist abgelaufen.                         | Das Databricks-Zugriffstoken ist standardmäßig 90 Tage gültig.  Erstellen Sie ein neues Token, und aktualisieren Sie den verknüpften Dienst. |
| 3201           | Fehlendes Pflichtfeld: settings.task.notebook_task.notebook_path | Fehlerhafte Erstellung: Notebook-Pfad wurde nicht ordnungsgemäß angegeben. | Geben Sie den Notebook-Pfad in der Databricks-Aktivität an. |
| 3201           | Cluster ... does not exist (Cluster ... ist nicht vorhanden)                                 | Erstellungsfehler: Databricks-Cluster ist nicht vorhanden oder wurde gelöscht. | Überprüfen Sie, ob der Databricks-Cluster vorhanden ist. |
| 3201           | Invalid python file URI: .... Please visit Databricks user guide for supported URI schemes. (Ungültiger URI für Python-Datei: .... Unterstützte URI-Schemas finden Sie im Databricks-Benutzerhandbuch.) | Fehlerhafte Erstellung                                                | Geben Sie entweder absolute Pfade für die Adressierungsschemas der Arbeitsbereiche oder „dbfs:/Ordner/Unterordner/foo.py“ für in DBFS gespeicherte Dateien an. |
| 3201           | {0} LinkedService should have domain and accessToken as required properties. („LinkedService“ muss „domain“ und „accessToken“ als erforderliche Eigenschaften enthalten.) | Fehlerhafte Erstellung                                                | Überprüfen Sie die [Definition des verknüpften Diensts](compute-linked-services.md#azure-databricks-linked-service). |
| 3201           | {0} LinkedService should specify either existing cluster Id or new cluster information for creation (LinkedService muss entweder die ID eines vorhandenen Clusters oder neue Clusterinformationen für die Erstellung angeben.) | Fehlerhafte Erstellung                                                | Überprüfen Sie die [Definition des verknüpften Diensts](compute-linked-services.md#azure-databricks-linked-service). |
| 3201           | Der Knotentyp „Standard_D16S_v3“ wird nicht unterstützt. Unterstützte Knotentypen:   Standard_DS3_v2, Standard_DS4_v2, Standard_DS5_v2, Standard_D8s_v3, Standard_D16s_v3, Standard_D32s_v3, Standard_D64s_v3, Standard_D3_v2, Standard_D8_v3, Standard_D16_v3, Standard_D32_v3, Standard_D64_v3, Standard_D12_v2, Standard_D13_v2, Standard_D14_v2, Standard_D15_v2, Standard_DS12_v2, Standard_DS13_v2, Standard_DS14_v2, Standard_DS15_v2, Standard_E8s_v3, Standard_E16s_v3, Standard_E32s_v3, Standard_E64s_v3, Standard_L4s, Standard_L8s, Standard_L16s, Standard_L32s, Standard_F4s, Standard_F8s, Standard_F16s, Standard_H16, Standard_F4s_v2, Standard_F8s_v2, Standard_F16s_v2, Standard_F32s_v2, Standard_F64s_v2, Standard_F72s_v2, Standard_NC12, Standard_NC24, Standard_NC6s_v3, Standard_NC12s_v3, Standard_NC24s_v3, Standard_L8s_v2, Standard_L16s_v2, Standard_L32s_v2, Standard_L64s_v2, Standard_L80s_v2 | Fehlerhafte Erstellung                                                | Siehe Fehlermeldung                                          |
| 3201           | Invalid notebook_path: .... Only absolute paths are currently supported. Paths must begin with '/'. (Ungültiger notebook_path: .... Derzeit werden nur absolute Pfade unterstützt. Pfade müssen mit ‚/‘ beginnen.) | Fehlerhafte Erstellung                                                | Siehe Fehlermeldung                                          |
| 3202           | There were already 1000 jobs created in past 3600 seconds, exceeding rate limit:   1000 job creations per 3600 seconds. (In den letzten 3.600 Sekunden wurden bereits 1.000 Aufträge erstellt. Erstellungslimit überschritten: 1.000 Auftragserstellungen pro 3.600 Sekunden.) | Zu viele Databricks-Ausführungen in einer Stunde.                         | Überprüfen Sie alle Pipelines, die den Databricks-Arbeitsbereich verwenden, auf die Auftragserstellungsrate.  Wenn Pipelines insgesamt zu viele Databricks-Ausführungen gestartet haben, migrieren Sie einige Pipelines zu einem neuen Arbeitsbereich. |
| 3202           | Could not parse request object: Expected 'key' and 'value' to be set for JSON map field base_parameters, got 'key: "..."' instead. (Anforderungsobjekt konnte nicht analysiert werden: Erwartet wurde, dass ‚key‘ und ‚value‘ für base_parameters des JSON-Zuordnungsfelds festgelegt werden. Stattdessen wurde ‚key: „...“‘ festgelegt.) | Erstellungsfehler: Kein Wert für den Parameter angegeben         | Überprüfen Sie den JSON-Code der Pipeline, und vergewissern Sie sich, dass für alle Parameter in baseParameters des Notebooks ein nicht leerer Wert angegeben ist. |
| 3202           | User: SimpleUserContext{userId=..., name=user@company.com, orgId=…} is not authorized to access cluster (Der Benutzer: ... ist nicht berechtigt, auf den Cluster zuzugreifen.) | Der Benutzer, der das Zugriffstoken generiert hat, ist nicht berechtigt, auf den im verknüpften Dienst angegebenen Databricks-Cluster zuzugreifen. | Stellen Sie sicher, dass der Benutzer über die erforderlichen Berechtigungen im Arbeitsbereich verfügt.   |
| 3203           | The cluster is in Terminated state, not available to receive jobs. Please fix the cluster or retry later. (Der Cluster hat den Status „Beendet“ und ist für den Empfang von Aufträgen nicht verfügbar. Beheben Sie den Cluster, oder versuchen Sie es später erneut.) | Der Cluster wurde beendet.    Für einen interaktiven Cluster ist dies möglicherweise eine Racebedingung. | Dies kann am besten durch Verwendung von Auftragsclustern vermieden werden.             |
| 3204           | Fehler bei der Auftragsausführung. Es können verschiedene Fehlermeldungen angezeigt werden, vom unerwarteten Clusterzustand bis hin zu aktivitätsspezifischen Meldungen.  Am häufigsten wird gar keine Fehlermeldung angezeigt. | –                                                          | –                                                          |



## <a name="azure-data-lake-analytics-u-sql"></a>Azure Data Lake Analytics (U-SQL)

| Fehlercode         | Fehlermeldung                                                | Problembeschreibung                                          | Mögliche Behebung / Empfohlene Aktion                             |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2709                 | The access token is from the wrong tenant (Das Zugriffstoken stammt vom falschen Mandanten)                    | Falscher AAD-Mandant                                         | Der für den Zugriff auf das ADLA-Konto verwendete Dienstprinzipal gehört zu einem anderen AAD-Mandanten. Erstellen Sie einen neuen Dienstprinzipal im gleichen Mandanten wie das ADLA-Konto. |
| 2711, 2705, 2704 | Unzulässig. Fehler bei der ACL-Überprüfung. Either the resource does not exist or the user is not authorized to perform the requested operation (Entweder ist die Ressource nicht vorhanden, oder der Benutzer ist nicht berechtigt, den angeforderten Vorgang auszuführen.)<br/><br/>User is not able to access to datalake store (Benutzer kann nicht auf Data Lake Store zugreifen.)  <br/><br/>User is not authorized to data lake analytics (Benutzer ist nicht für Data Lake Analytics autorisiert.) | Der angegebene Dienstprinzipal oder das angegebene Zertifikat hat keinen Zugriff auf die Datei im Speicher. | Vergewissern Sie sich, dass der Dienstprinzipal oder das Zertifikat, die für ADLA-Aufträge angegeben werden, Zugriff auf das ADLA-Konto und den zugehörigen ADLS-Standardspeicher im Stammordner haben. |
| 2711                 | Die Datei oder der Ordner „Azure Data Lake Store“ kann nicht gefunden werden.       | Der Pfad zur USQL-Datei ist falsch, oder die Anmeldeinformationen des verknüpften Diensts haben keinen Zugriff. | Überprüfen Sie den Pfad und die im verknüpften Dienst angegebenen Anmeldeinformationen. |
| 2707                 | Cannot resolve the account of AzureDataLakeAnalytics. Please check 'AccountName' and 'DataLakeAnalyticsUri'. (Das Konto von AzureDataLakeAnalytics kann nicht aufgelöst werden. Überprüfen Sie „AccountName“ und „DataLakeAnalyticsUri“.) | Das ADLA-Konto im verknüpften Dienst ist falsch.                  | Stellen Sie sicher, dass das richtige Konto angegeben wurde.             |
| 2703                 | Fehler-ID: E_CQO_SYSTEM_INTERNAL_ERROR oder jeder Fehler, der mit „Fehler-ID:“ beginnt | Der Fehler stammt von ADLA.                                    | Alle Fehlermeldungen, die dem Beispiel ähneln, geben an, dass der Auftrag an ADLA übermittelt wurde und dort bei dem Skript Fehler aufgetreten sind. Die Überprüfung muss in ADLA erfolgen. Öffnen Sie das Portal, und navigieren Sie zum ADLA-Konto. Suchen Sie nach dem Auftrag mithilfe der Ausführungs-ID der ADF-Aktivität (nicht mithilfe der Ausführungs-ID der Pipeline). Bei diesem Auftrag finden Sie weitere Informationen zu dem Fehler und der entsprechenden Problembehandlung. Wenn die Lösung nicht klar ist, wenden Sie sich an das ADLA-Supportteam, und geben Sie die Auftrags-ID mit Ihrem Kontonamen und der Auftrags-ID an. |
| 2709                 | We cannot accept your job at this moment. The maximum number of queued jobs for your account is 200. (Der Auftrag kann derzeit nicht angenommen werden. Die maximale Anzahl der Aufträge in der Warteschlange für Ihr Konto beträgt 200.) | Drosselung in ADLA                                           | Reduzieren Sie die Anzahl der an ADLA übermittelten Aufträge, indem Sie die ADF-Trigger und Parallelitätseinstellungen für Aktivitäten ändern, oder erhöhen Sie die Grenzwerte für ADLA. |
| 2709                 | This job was rejected because it requires 24 AUs and this account has an administrator-defined policy that prevents a job from using more than 5 AUs. (Dieser Auftrag wurde abgelehnt, da er 24 Analytics-Einheiten erfordert und dieses Konto eine vom Administrator definierte Richtlinie enthält, die verhindert, dass für einen Auftrag mehr als 5 Analytics-Einheiten verwendet werden.) | Drosselung in ADLA                                           | Reduzieren Sie die Anzahl der an ADLA übermittelten Aufträge, indem Sie die ADF-Trigger und Parallelitätseinstellungen für Aktivitäten ändern, oder erhöhen Sie die Grenzwerte für ADLA. |



## <a name="azure-functions"></a>Azure-Funktionen

| Fehlercode | Fehlermeldung                           | BESCHREIBUNG                                                  | Mögliche Behebung / Empfohlene Aktion                           |
| ------------ | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 3600         | Response Content is not a valid JObject (Antwortinhalt ist kein gültiges JObject) | Das bedeutet, dass die aufgerufene Azure-Funktion keine JSON-Nutzlast in der Antwort zurückgegeben hat. Die ADF-Aktivität „Azure-Funktion“ unterstützt nur JSON-Antwortinhalte. | Aktualisieren Sie die Azure-Funktion so, dass eine gültige JSON-Nutzlast zurückgegeben wird, eine C#-Funktion kann z. B. (ActionResult)new<OkObjectResult("{`\"Id\":\"123\"`}"); zurückgeben. |
| 3600         | Ungültige HttpMethod: ‚...‘.               | Die in der Nutzlast der Aktivität angegebene HTTP-Methode wird von der Aktivität „Azure-Funktion“ nicht unterstützt. | Folgende HTTP-Methoden werden unterstützt:  <br/>PUT, POST, GET, DELETE, OPTIONS, HEAD, TRACE |



## <a name="custom-azure-batch"></a>Benutzerdefiniert (Azure Batch)
| Fehlercode | Fehlermeldung                                                | BESCHREIBUNG                                                  | Mögliche Behebung / Empfohlene Aktion                           |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2500         | Hit unexpected exception and execution failed. (Unerwartete Ausnahme, Fehler bei der Ausführung.)             | Der Befehl kann nicht gestartet werden, oder das Programm hat einen Fehlercode zurückgegeben. | Überprüfen Sie, ob die ausführbare Datei vorhanden ist. Wenn das Programm gestartet wurde, überprüfen Sie die in das Speicherkonto hochgeladenen Dateien „stdout.txt“ und „stderr.txt“. Es empfiehlt sich, umfangreiche Protokolle im Code zum Debuggen auszugeben. |
| 2501         | Can not access user batch account, please check batch account settings (Auf das Batch-Konto des Benutzers kann nicht zugegriffen werden. Überprüfen Sie die Einstellungen des Batch-Kontos.) | Falscher Batch-Zugriffsschlüssel oder Poolname angegeben.            | Der Poolname und der Batch-Zugriffsschlüssel im verknüpften Dienst müssen überprüft werden. |
| 2502         | Can not access user storage account, please check storage account settings. (Auf das Speicherkonto des Benutzers kann nicht zugegriffen werden. Überprüfen Sie die Einstellungen des Speicherkontos.) | Falscher Speicherkontoname oder Zugriffsschlüssel angegeben.       | Der Speicherkontoname und der Zugriffsschlüssel im verknüpften Dienst müssen überprüft werden. |
| 2504         | Operation returned an invalid status code 'BadRequest' (Vorgang hat den ungültigen Statuscode ‚BadRequest‘ zurückgegeben)     | Zu viele Dateien in „folderPath“ in der benutzerdefinierten Aktivität.  (Die Gesamtgröße von „resourceFiles“ darf nicht mehr als 32.768 Zeichen sein.) | Entfernen Sie nicht benötigte Dateien, oder zippen Sie sie, und fügen Sie einen unzip-Befehl zum Extrahieren hinzu, z. B.: powershell.exe -nologo -noprofile -command "& { Add-Type -A 'System.IO.Compression.FileSystem'; [IO.Compression.ZipFile]::ExtractToDirectory($zipFile, $folder); }" ; $folder\IhrProgramm.exe |
| 2505         | Eine SAS (Shared Access Signature) kann nur erstellt werden, wenn die Kontoschlüssel-Anmeldeinformationen verwendet werden. | Benutzerdefinierte Aktivitäten unterstützen nur Speicherkonten, die einen Zugriffsschlüssel verwenden. | Siehe Beschreibung                                            |
| 2507         | The folder path does not exist or is empty: ... (Der Ordnerpfad ist nicht vorhanden oder leer: ...)            | Unter dem angegebenen Pfad sind im Speicherkonto keine Dateien vorhanden.       | „folderPath“ muss die ausführbaren Dateien enthalten, die Sie ausführen möchten. |
| 2508         | There’re duplicate files in resource folder. (Der Ressourcenordner enthält doppelte Dateien.)               | Es sind mehrere Dateien mit dem gleichen Namen in verschiedenen Unterordnern von „folderPath“ vorhanden. | Benutzerdefinierte Aktivitäten vereinfachen die Ordnerstruktur unter „folderPath“.  Wenn die Ordnerstruktur beibehalten werden muss, zippen Sie die Dateien, und extrahieren Sie sie mit einem unzip-Befehl in Azure Batch, z. B.: powershell.exe -nologo -noprofile -command "& { Add-Type -A 'System.IO.Compression.FileSystem'; [IO.Compression.ZipFile]::ExtractToDirectory($zipFile, $folder); }" ; $folder\IhrProgramm.exe |
| 2509         | Batch url ... is invalid, it must be in Uri format. (Batch-URL ... ist ungültig, die Angabe muss im URI-Format erfolgen.)         | Batch-URLs müssen https://mybatchaccount.eastus.batch.azure.com ähneln. | Siehe Beschreibung                                            |
| 2510         | Fehler beim Senden der Anforderung.               | Die Batch-URL ist ungültig.                                         | Überprüfen Sie die Batch-URL.                                            |

## <a name="hdinsight-spark-hive-mapreduce-pig-hadoop-streaming"></a>HDInsight (Spark, Hive, MapReduce, Pig, Hadoop Streaming)

| Fehlercode | Fehlermeldung                                                | BESCHREIBUNG                                                  | Mögliche Behebung / Empfohlene Aktion                           |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2300, 2310 | Hadoop job submission failed. (Fehler beim Übermitteln des Hadoop-Auftrags.) Fehler Der Remotename konnte nicht aufgelöst werden.“ verursacht. <br/><br/>Der Cluster wurde nicht gefunden. | Der angegebene Cluster-URI ist ungültig.                              | Stellen Sie sicher, dass der Cluster nicht gelöscht wurde und der angegebene URI richtig ist. Sie können den URI in jedem Browser öffnen. Daraufhin sollte die Ambari-Benutzeroberfläche angezeigt werden. Wenn sich der Cluster in einem VNET befindet, muss der URI der private URI sein, und Sie sollten den Cluster über einen virtuellen Computer öffnen, der zum gleichen VNET gehört. Weitere Informationen zum [virtuellen Netzwerk in HDInsight](https://docs.microsoft.com/azure/hdinsight/hdinsight-extend-hadoop-virtual-network#directly-connect-to-apache-hadoop-services). |
| 2\.300         | Hadoop job submission failed. Job: …, Cluster: …/. (Fehler beim Übermitteln des Hadoop-Auftrags. Auftrag: …, Cluster: …/.) Fehler Eine Aufgabe wurde abgebrochen. | Zeitüberschreitung bei der Übermittlung des Auftrags.                         | Dies kann entweder ein allgemeines HDInsight-Verbindungsproblem oder ein Netzwerkverbindungsproblem sein. Überprüfen Sie zunächst, ob die HDInsight Ambari-Benutzeroberfläche über einen Browser verfügbar ist und ob Ihre Anmeldeinformationen noch gültig sind. Überprüfen Sie dies über den virtuellen oder physischen Computer, auf dem die selbstgehostete IR installiert ist, wenn die selbstgehostete IR verwendet wird. Übermitteln Sie dann den Auftrag erneut über ADF. Wenn weiterhin Fehler auftreten, wenden Sie sich an das ADF-Team. |
| 2\.300         | Nicht autorisiert:   Der Ambari-Benutzername oder das Kennwort ist falsch.  <br/><br/>Nicht autorisiert:   User admin is locked out in Ambari (Nicht autorisiert: Benutzeradministrator ist in Ambari gesperrt.)   <br/><br/>403 – Unzulässig: Zugriff verweigert | Die angegebenen Anmeldeinformationen für HDInsight sind falsch oder abgelaufen. | Korrigieren Sie sie, und stellen Sie den verknüpften Dienst erneut bereit. Stellen Sie zunächst sicher, dass die Anmeldeinformationen in HDInsight funktionieren. Öffnen Sie dazu den Cluster-URI in einem Browser, und versuchen Sie sich anzumelden. Wenn sie nicht funktionieren, können Sie sie im Azure-Portal zurücksetzen. |
| 2300, 2310 | 502 – Webserver hat als Gateway oder Proxyserver eine ungültige Antwort erhalten.       <br/>Ungültiges Gateway | Der Fehler stammt von HDInsight.                               | Dieser Fehler stammt vom HDInsight-Cluster. Informationen zu allgemeinen Fehlern finden Sie unter [HDInsight-Problembehandlung](https://hdinsight.github.io/ambari/ambari-ui-502-error.html).    <br/>Bei Spark-Clustern kann dies auch auf [diese Ursache](https://hdinsight.github.io/spark/spark-thriftserver-errors.html) zurückzuführen sein. <br/><br/>[Zusätzlicher Link](https://docs.microsoft.com/azure/application-gateway/application-gateway-troubleshooting-502) |
| 2\.300         | Hadoop job submission failed. Job: …, Cluster: ... Error: {\"error\":\"Unable to service the submit job request as templeton service is busy with too many submit job requests. Please wait for some time before retrying the operation. Please refer to the config templeton.parallellism.job.submit to configure concurrent requests.\ (Fehler beim Übermitteln des Hadoop-Auftrags. Auftrag: …, Cluster: ... Fehler: Die Anforderung zur Auftragsübermittlung kann nicht verarbeitet werden, da der Templeton-Dienst mit zu vielen Anforderungen zur Auftragsübermittlung ausgelastet ist. Warten Sie einen Moment, bevor Sie den Vorgang wiederholen. Informationen zum Konfigurieren von gleichzeitigen Anforderungen finden Sie in der Konfiguration „templeton.parallellism.job.submit“.\)  <br/><br/>Hadoop job submission failed. Job: 161da5d4-6fa8-4ef4-a240-6b6428c5ae2f, Cluster: https://abc-analytics-prod-hdi-hd-trax-prod01.azurehdinsight.net/.   Error: {\"error\":\"java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1561147195099_3730 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.joblauncher already has 500 applications, cannot accept submission of application: application_1561147195099_3730\ (Fehler beim Übermitteln des Hadoop-Auftrags. Auftrag: 161da5d4-6fa8-4ef4-a240-6b6428c5ae2f, 161da5d4-6fa8-4ef4-a240-6b6428c5ae2f, Cluster: ... Fehler: {...java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Fehler beim Übermitteln von application_1561147195099_3730 an YARN: org.apache.hadoop.security.AccessControlException: Warteschlange „root.joblauncher“ enthält bereits 500 Anwendungen, Übermittlung der Anwendung kann nicht angenommen werden: application_1561147195099_3730\) | Zu viele Aufträge werden gleichzeitig an HDInsight übermittelt. | Begrenzen Sie die Anzahl der Aufträge, die gleichzeitig an HDI übermittelt werden. Wenn die Aufträge über die gleiche Aktivität übermittelt werden, finden Sie entsprechende Informationen in der ADF-Aktivität für Parallelität. Ändern Sie die Trigger so, dass die gleichzeitigen Pipelineausführungen über die Zeit verteilt werden. Informationen zum Optimieren von „templeton.parallellism.job.submit“, wie in der Fehlermeldung empfohlen, finden Sie in der HDInsight-Dokumentation. |
| 2303, 2347 | Hadoop job failed with exit code '5'. See 'wasbs://adfjobs@adftrialrun.blob.core.windows.net/StreamingJobs/da4afc6d-7836-444e-bbd5-635fce315997/18_06_2019_05_36_05_050/stderr' for more details. (Fehler beim Hadoop-Auftrag mit Exitcode ‚5‘. Weitere Informationen finden Sie unter ‚...‘.)  <br/><br/>Hive execution failed with error code 'UserErrorHiveOdbcCommandExecutionFailure'.   See 'wasbs://adfjobs@eclsupplychainblobd.blob.core.windows.net/HiveQueryJobs/16439742-edd5-4efe-adf6-9b8ff5770beb/18_06_2019_07_37_50_477/Status/hive.out' for more details (Fehler bei der Hive-Ausführung mit dem Fehlercode ‚UserErrorHiveOdbcCommandExecutionFailure‘. Weitere Informationen finden Sie unter ‚...‘..) | Der Auftrag wurde an HDInsight übermittelt, und in HDInsight ist ein Fehler aufgetreten. | Der Auftrag wurde erfolgreich an HDInsight übermittelt. Bei seiner Ausführung ist ein Fehler im Cluster aufgetreten. Öffnen Sie den Auftrag in der HDInsight Ambari-Benutzeroberfläche, und öffnen Sie dort die Protokolle, oder öffnen Sie die Datei im Speicher, wie in der Fehlermeldung angegeben. In dieser Datei sind die Details zu dem Fehler angegeben. |
| 2328         | Internal server error occurred while processing the request. Please retry the request or contact support (Beim Verarbeiten der Anforderung ist ein interner Serverfehler aufgetreten. Wiederholen Sie die Anforderung, oder wenden Sie sich an den Support.) | Tritt bei HDInsight on Demand auf.                              | Dieser Fehler stammt vom HDInsight-Dienst, wenn bei der HDInsight-Bereitstellung Fehler aufgetreten sind. Wenden Sie sich an das HDInsight-Team, und geben Sie den Namen des On-Demand-Clusters an. |
| 2310         | java.lang.NullPointerException                               | Der Fehler ist beim Übermitteln des Auftrags an den Spark-Cluster aufgetreten.      | Diese Ausnahme stammt von HDInsight und verbirgt das eigentliche Problem.   Wenden Sie sich an das HDInsight-Team, und geben Sie den Clusternamen und den Zeitraum der Aktivitätsausführung an. |
|              | Alle anderen Fehler                                             |                                                              | Entsprechende Informationen finden Sie unter [HDInsight-Problembehandlung](../hdinsight/hdinsight-troubleshoot-guide.md) und [HDInsight FAQ](https://hdinsight.github.io/) (Häufig gestellte Fragen zu HDInsight). |



## <a name="web-activity"></a>Webaktivität

| Fehlercode | Fehlermeldung                                                | BESCHREIBUNG                                                  | Mögliche Behebung / Empfohlene Aktion                           |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2108         | Ungültige HttpMethod: ‚...‘.                                    | Die in der Nutzlast der Aktivität angegebene HTTP-Methode wird von der Webaktivität nicht unterstützt. | Folgende HTTP-Methoden werden unterstützt: <br/>PUT, POST, GET, DELETE |
| 2108         | Invalid Server Error 500 (Fehler: Ungültiger Server 500)                                     | Interner Fehler am Endpunkt                               | Überprüfen Sie die Funktionalität für die URL (mit Fiddler/Postman): [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | Nicht autorisiert 401                                             | Fehlende gültige Authentifizierung für die Anforderung                      | Geben Sie eine gültige Authentifizierungsmethode an (Token ist möglicherweise abgelaufen).   <br/><br/>Überprüfen Sie die Funktionalität für die URL (mit Fiddler/Postman): [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | Unzulässig 403                                                | Fehlende erforderliche Berechtigungen                                 | Überprüfen Sie die Benutzerberechtigungen für die Ressource, auf die zugegriffen wird.   <br/><br/>Überprüfen Sie die Funktionalität für die URL (mit Fiddler/Postman): [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | Ungültige Anforderung 400                                              | Ungültige HTTP-Anforderung                                         | Überprüfen Sie die URL, das Verb und den Text der Anforderung.   <br/><br/>Verwenden Sie Fiddler/Postman, um die Anforderung zu überprüfen: [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | Nicht gefunden – 404                                                | Ressource wurde nicht gefunden.                                       | Verwenden Sie Fiddler/Postman, um die Anforderung zu überprüfen: [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | Dienst nicht verfügbar                                          | Dienst nicht verfügbar                                       | Verwenden Sie Fiddler/Postman, um die Anforderung zu überprüfen: [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | Nicht unterstützter Medientyp                                       | Content-Type stimmt nicht mit dem Text der Webaktivität überein.           | Geben Sie den richtigen Content-Type an, der mit dem Nutzlastformat übereinstimmt. Verwenden Sie Fiddler/Postman, um die Anforderung zu überprüfen: [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | Die gesuchte Ressource wurde entfernt oder umbenannt, oder sie steht vorübergehend nicht zur Verfügung. | Die Ressource ist nicht verfügbar.                                | Verwenden Sie Fiddler/Postman, um den Endpunkt zu überprüfen: [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | Die gesuchte Seite kann nicht angezeigt werden, da für den Zugriff eine ungültige Methode (HTTP-Verb) verwendet wird. | Eine falsche Webaktivitätsmethode wurde in der Anforderung angegeben.   | Verwenden Sie Fiddler/Postman, um den Endpunkt zu überprüfen: [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |
| 2108         | invalid_payload                                              | Falscher Text für die Webaktivität                       | Verwenden Sie Fiddler/Postman, um den Endpunkt zu überprüfen: [Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung](#how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application) |

#### <a name="how-to-use-fiddler-to-create-an-http-session-of-the-monitored-web-application"></a>Verwenden von Fiddler zum Erstellen einer HTTP-Sitzung der überwachten Webanwendung

1. Laden Sie das Tool [Fiddler](https://www.telerik.com/download/fiddler) herunter, und installieren Sie es.

2. Wenn in der Webanwendung HTTPS verwendet wird:

   1. Öffnen Sie Fiddler.

   2. Navigieren Sie zu **Tools > Fiddler Options** (Tools > Fiddler-Optionen), und wählen Sie die im Screenshot gezeigten Optionen aus. 

      ![Fiddler-Optionen](media/data-factory-troubleshoot-guide/fiddler-options.png)

3. Wenn in der Anwendung SSL-Zertifikate verwendet werden, müssen Sie außerdem das Fiddler-Zertifikat auf Ihrem Gerät hinzufügen.

4. Um das Fiddler-Zertifikat auf Ihrem Gerät hinzuzufügen, navigieren Sie zu **Tools** > **Fiddler Options** > **HTTPS** > **Actions** > **Export Root Certificate to Desktop**  (Tools > Fiddler-Optionen > HTTPS > Aktionen > Stammzertifikat auf Desktop exportieren), und rufen Sie das Fiddler-Zertifikat ab.

5. Deaktivieren Sie die Erfassung des Datenverkehrs, sodass der Browsercache gelöscht werden kann, um eine neue Ablaufverfolgung zu starten.

6. 1. Navigieren Sie zu **File** > **Capture Traffic** (Datei > Datenverkehr erfassen), oder drücken Sie **F12**.
   2. Löschen Sie den Browsercache, sodass alle zwischengespeicherten Elemente gelöscht werden und erneut heruntergeladen werden müssen.

7. Erstellen Sie die Anforderung: 

8. 1. Klicken Sie auf die Registerkarte „Composer“.
   2. Legen Sie die HTTP-Methode und die HTTP-URL fest.
   3. Fügen Sie bei Bedarf Header und den Anforderungstext hinzu.
   4. Klicken Sie auf „Execute“ (Ausführen).

9. Starten Sie die Erfassung des Datenverkehrs wieder, und schließen Sie die entsprechende Transaktion auf Ihrer Seite ab.

10. Navigieren Sie anschließend zu **File** > **Save** > **All Sessions** (Datei > Speichern > Alle Sitzungen).

Weitere Informationen zu Fiddler finden Sie [hier](https://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureFiddler).

## <a name="next-steps"></a>Nächste Schritte

Für weitere Hilfe bei der Suche nach einer Lösung für Ihr Problem, stehen Ihnen hier weitere Ressourcen zur Verfügung.

*  [Blogs](https://azure.microsoft.com/blog/tag/azure-data-factory/)
*  [Funktionsanfragen](https://feedback.azure.com/forums/270578-data-factory)
*  [Videos](https://azure.microsoft.com/resources/videos/index/?sort=newest&services=data-factory)
*  [MSDN-Forum](https://social.msdn.microsoft.com/Forums/home?sort=relevancedesc&brandIgnore=True&searchTerm=data+factory)
*  [Stack Overflow](https://stackoverflow.com/questions/tagged/azure-data-factory)
*  [Twitter](https://twitter.com/hashtag/DataFactory)



