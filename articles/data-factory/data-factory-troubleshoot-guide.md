---
title: Troubleshooting für Azure Data Factory | Microsoft-Dokumentation
description: Erfahren Sie, wie Sie in der Azure Data Factory Fehler bei externen Steuerungsaktivitäten beheben können.
services: data-factory
author: nabhishek
ms.service: data-factory
ms.topic: troubleshooting
ms.date: 6/26/2019
ms.author: abnarain
ms.reviewer: craigg
ms.openlocfilehash: 1995ce2a91bfbc115f80c99687cc84b52ef614ec
ms.sourcegitcommit: 78ebf29ee6be84b415c558f43d34cbe1bcc0b38a
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 08/12/2019
ms.locfileid: "68950113"
---
# <a name="troubleshoot-azure-data-factory"></a>Troubleshooting für Azure Data Factory

In diesem Artikel werden gängige Methoden zur Problembehandlung für externe Steuerungsaktivitäten in Azure Data Factory erläutert.

## <a name="azure-databricks"></a>Azure Databricks

| Fehlercode | Fehlermeldung                                          | BESCHREIBUNG                             | Empfehlung                             |
| -------------- | ----------------------------------------------------- | --------------------------------------------------------------| :----------------------------------------------------------- |
| 3200           | Fehler 403.                                                    | Das Databricks-Zugriffstoken ist abgelaufen.                         | Das Databricks-Zugriffstoken ist standardmäßig 90 Tage gültig.  Erstellen Sie ein neues Token, und aktualisieren Sie den verknüpften Dienst. |
| 3201           | Fehlendes Pflichtfeld: settings.task.notebook_task.notebook_path | Fehlerhafte Erstellung: Notebook-Pfad wurde nicht ordnungsgemäß angegeben. | Geben Sie den Notebook-Pfad in der Databricks-Aktivität an. |
| 3201           | Cluster ... does not exist (Cluster ... ist nicht vorhanden).                                 | Erstellungsfehler: Databricks-Cluster ist nicht vorhanden oder wurde gelöscht. | Überprüfen Sie, ob der Databricks-Cluster vorhanden ist. |
| 3201           | Invalid Python file URI... (Ungültige Python-Datei-URI...) Please visit Databricks user guide for supported URI schemes. (Ungültiger URI für Python-Datei: .... Unterstützte URI-Schemas finden Sie im Databricks-Benutzerhandbuch.) | Fehlerhafte Erstellung.                                                | Geben Sie entweder absolute Pfade für die Adressierungsschemas der Arbeitsbereiche oder „`dbfs:/folder/subfolder/foo.py`“ für im Databricks-Dateisystem gespeicherte Dateien an. |
| 3201           | {0} LinkedService should have domain and accessToken as required properties. („LinkedService“ muss „domain“ und „accessToken“ als erforderliche Eigenschaften enthalten.) | Fehlerhafte Erstellung.                                                | Überprüfen Sie die [Definition des verknüpften Diensts](compute-linked-services.md#azure-databricks-linked-service). |
| 3201           | {0} LinkedService should specify either existing cluster ID or new cluster information for creation (LinkedService muss entweder die ID eines vorhandenen Clusters oder neue Clusterinformationen für die Erstellung angeben.) | Fehlerhafte Erstellung.                                                | Überprüfen Sie die [Definition des verknüpften Diensts](compute-linked-services.md#azure-databricks-linked-service). |
| 3201           | Der Knotentyp „Standard_D16S_v3“ wird nicht unterstützt. Unterstützte Knotentypen:   Standard_DS3_v2, Standard_DS4_v2, Standard_DS5_v2, Standard_D8s_v3, Standard_D16s_v3, Standard_D32s_v3, Standard_D64s_v3, Standard_D3_v2, Standard_D8_v3, Standard_D16_v3, Standard_D32_v3, Standard_D64_v3, Standard_D12_v2, Standard_D13_v2, Standard_D14_v2, Standard_D15_v2, Standard_DS12_v2, Standard_DS13_v2, Standard_DS14_v2, Standard_DS15_v2, Standard_E8s_v3, Standard_E16s_v3, Standard_E32s_v3, Standard_E64s_v3, Standard_L4s, Standard_L8s, Standard_L16s, Standard_L32s, Standard_F4s, Standard_F8s, Standard_F16s, Standard_H16, Standard_F4s_v2, Standard_F8s_v2, Standard_F16s_v2, Standard_F32s_v2, Standard_F64s_v2, Standard_F72s_v2, Standard_NC12, Standard_NC24, Standard_NC6s_v3, Standard_NC12s_v3, Standard_NC24s_v3, Standard_L8s_v2, Standard_L16s_v2, Standard_L32s_v2, Standard_L64s_v2, Standard_L80s_v2. | Fehlerhafte Erstellung.                                                | Weitere Informationen erhalten Sie in der Fehlermeldung.                                          |
| 3201           | Invalid notebook_path:... (Ungültiger notebook_path:...) Only absolute paths are currently supported. Paths must begin with '/'. (Ungültiger notebook_path: .... Derzeit werden nur absolute Pfade unterstützt. Pfade müssen mit ‚/‘ beginnen.) | Fehlerhafte Erstellung.                                                | Weitere Informationen finden Sie in der Fehlermeldung.                                          |
| 3202           | There were already 1000 jobs created in past 3600 seconds, exceeding rate limit:   1000 job creations per 3600 seconds. (In den letzten 3.600 Sekunden wurden bereits 1.000 Aufträge erstellt. Erstellungslimit überschritten: 1.000 Auftragserstellungen pro 3.600 Sekunden.) | Zu viele Databricks-Ausführungen in einer Stunde.                         | Überprüfen Sie alle Pipelines, die den Databricks-Arbeitsbereich verwenden, auf ihre Auftragserstellungsrate.  Wenn Pipelines insgesamt zu viele Databricks-Ausführungen gestartet haben, migrieren Sie einige Pipelines zu einem neuen Arbeitsbereich. |
| 3202           | Could not parse request object: Expected 'key' and 'value' to be set for JSON map field base_parameters, got 'key: "..."' instead. (Anforderungsobjekt konnte nicht analysiert werden: Erwartet wurde, dass ‚key‘ und ‚value‘ für base_parameters des JSON-Zuordnungsfelds festgelegt werden. Stattdessen wurde ‚key: „...“‘ festgelegt.) | Erstellungsfehler: Kein Wert für den Parameter angegeben.         | Überprüfen Sie den JSON-Code der Pipeline, und vergewissern Sie sich, dass für alle Parameter in baseParameters des Notebooks ein nicht leerer Wert angegeben ist. |
| 3202           | User: `SimpleUserContext{userId=..., name=user@company.com, orgId=...}` is not authorized to access cluster (Der Benutzer: ... ist nicht berechtigt, auf den Cluster zuzugreifen). | Der Benutzer, der das Zugriffstoken generiert hat, ist nicht berechtigt, auf den im verknüpften Dienst angegebenen Databricks-Cluster zuzugreifen. | Stellen Sie sicher, dass der Benutzer über die erforderlichen Berechtigungen im Arbeitsbereich verfügt.   |
| 3203           | The cluster is in Terminated state, not available to receive jobs. Please fix the cluster or retry later. (Der Cluster hat den Status „Beendet“ und ist für den Empfang von Aufträgen nicht verfügbar. Beheben Sie den Cluster, oder versuchen Sie es später erneut.) | Der Cluster wurde beendet.    Für interaktive Cluster ist dies möglicherweise eine Racebedingung. | Die beste Möglichkeit, dies zu vermeiden, ist die Verwendung von Auftragsclustern.             |
| 3204           | Fehler bei der Auftragsausführung.  | Fehlermeldungen weisen auf verschiedene Probleme hin, wie z. B. einen unerwarteten Clusterzustand oder eine bestimmte Aktivität. Meistens erscheint überhaupt keine Fehlermeldung.                                                          | –                                                          |



## <a name="azure-data-lake-analytics"></a>Azure Data Lake Analytics

Die folgende Tabelle gilt für U-SQL.

| Fehlercode         | Fehlermeldung                                                | BESCHREIBUNG                                          | Empfehlung                            |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2709                 | The access token is from the wrong tenant (Das Zugriffstoken stammt vom falschen Mandanten).                    | Fehlerhafter Azure Active Directory-Mandanten (Azure AD).                                         | Der Dienstprinzipal, der für den Zugriff auf Azure Data Lake Analytics verwendet wird, gehört einem anderen Azure AD-Mandanten. Erstellen Sie einen neuen Dienstprinzipal im selben Mandanten wie das Data Lake Analytics-Konto. |
| 2711, 2705, 2704 | Unzulässig. Fehler bei der ACL-Überprüfung. Either the resource does not exist or the user is not authorized to perform the requested operation (Entweder ist die Ressource nicht vorhanden, oder der Benutzer ist nicht berechtigt, den angeforderten Vorgang auszuführen.).<br/><br/>User is not able to access to Data Lake Store (Benutzer kann nicht auf Data Lake Store zugreifen.)  <br/><br/>User is not authorized to Data Lake Analytics (Benutzer ist nicht für Data Lake Analytics autorisiert.) | Der angegebene Dienstprinzipal oder das angegebene Zertifikat hat keinen Zugriff auf die Datei im Speicher. | Stellen Sie sicher, dass der Dienstprinzipal oder das Zertifikat, das der Benutzer für Data Lake Analytics-Aufträge bereitstellt, Zugriff auf das Data Lake Analytics-Konto und die Standard-Data Lake Storage-Instanz aus dem Stammordner hat. |
| 2711                 | Cannot find the 'Azure Data Lake Store' file or folder (Die Datei oder der Ordner „Azure Data Lake Store“ kann nicht gefunden werden.).       | The path to the U-SQL file is wrong, or the linked service credentials don't have access (Der Pfad zur USQL-Datei ist falsch, oder die Anmeldeinformationen des verknüpften Diensts haben keinen Zugriff.). | Überprüfen Sie den Pfad und die im verknüpften Dienst angegebenen Anmeldeinformationen. |
| 2707                 | Cannot resolve the account of AzureDataLakeAnalytics. Please check 'AccountName' and 'DataLakeAnalyticsUri'. (Das Konto von AzureDataLakeAnalytics kann nicht aufgelöst werden. Überprüfen Sie „AccountName“ und „DataLakeAnalyticsUri“.) | Das Data Lake Analytics-Konto im verlinkten Dienst ist falsch.                  | Stellen Sie sicher, dass das richtige Konto angegeben wurde.             |
| 2703                 | Fehler-ID: E_CQO_SYSTEM_INTERNAL_ERROR (oder jeder Fehler, der mit „Fehler-ID:“ beginnt). | Dieser Fehler stammt aus Data Lake Analytics.                                    | Alle Fehlermeldungen, die dem Beispiel ähneln, geben an, dass der Auftrag an Data Lake Analytics übermittelt wurde und dort bei dem Skript Fehler aufgetreten sind. Untersuchen Sie in Data Lake Analytics. Gehen Sie im Portal zum Data Lake Analytics-Konto und suchen Sie nach dem Auftrag, indem Sie die Aktivitätsausführungs-ID von Data Factory verwenden (nicht die Pipelineausführungs-ID). Bei diesem Auftrag finden Sie weitere Informationen zu dem Fehler und der entsprechenden Problembehandlung. Wenn die Lösung nicht klar ist, wenden Sie sich an das Data Lake Analytics-Supportteam, und geben Sie die Auftrags-ID mit Ihrem Kontonamen und der Auftrags-ID an. |
| 2709                 | We cannot accept your job at this moment. The maximum number of queued jobs for your account is 200. (Der Auftrag kann derzeit nicht angenommen werden. Die maximale Anzahl der Aufträge in der Warteschlange für Ihr Konto beträgt 200.) | Dieser Fehler wird durch die Drosselung von Data Lake Analytics verursacht.                                           | Reduzieren Sie die Anzahl der an Data Lake Analytics übermittelten Aufträge, indem Sie die Data Factory-Trigger und Parallelitätseinstellungen für Aktivitäten ändern. Alternativ können Sie auch die Grenzwerte für Data Lake Analytics erhöhen. |
| 2709                 | This job was rejected because it requires 24 AUs. (Dieser Auftrag wurde zurückgewiesen, weil er 24 AUs erfordert.) Die vom Administrator definierte Richtlinie dieses Kontos verhindert, dass ein Auftrag mehr als 5 AUs verwendet. | Dieser Fehler wird durch die Drosselung von Data Lake Analytics verursacht.                                           | Reduzieren Sie die Anzahl der an Data Lake Analytics übermittelten Aufträge, indem Sie die Data Factory-Trigger und Parallelitätseinstellungen für Aktivitäten ändern. Alternativ können Sie auch die Grenzwerte für Data Lake Analytics erhöhen. |



## <a name="azure-functions"></a>Azure-Funktionen

| Fehlercode | Fehlermeldung                           | BESCHREIBUNG                                                  | Empfehlung                           |
| ------------ | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 3600         | Response Content is not a valid JObject (Antwortinhalt ist kein gültiges JObject). | Die aufgerufene Azure-Funktion hat keine JSON-Nutzlast in der Antwort zurückgegeben. Die „Azure-Funktion“-Aktivität in Data Factory unterstützt nur JSON-Antwortinhalte. | Aktualisieren Sie die Azure-Funktion, um eine gültige JSON-Payload zurückzugeben. Beispielsweise kann eine C#-Funktion `(ActionResult)new<OkObjectResult("{`\"Id\":\"123\"`}");` zurückgeben. |
| 3600         | Invalid HttpMethod: '...' (Ungültige HttpMethod: ‚...‘.)               | Die in der Aktivitäts-Payload angegebene HTTP-Methode wird von der Aktivität „Azure-Funktion“ nicht unterstützt. | Verwenden Sie eine unterstützte HTTP-Methode wie PUT, POST, GET, DELETE, OPTIONS, HEAD oder TRACE. |



## <a name="custom"></a>Benutzerdefiniert

Die folgende Tabelle gilt für Azure Batch.

| Fehlercode | Fehlermeldung                                                | BESCHREIBUNG                                                  | Empfehlung                          |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2500         | Hit unexpected exception and execution failed. (Unerwartete Ausnahme, Fehler bei der Ausführung.)             | Der Befehl kann nicht gestartet werden, oder das Programm hat einen Fehlercode zurückgegeben. | Stellen Sie sicher, dass die ausführbare Datei vorhanden ist. Wenn das Programm gestartet wurde, stellen Sie sicher, dass die Dateien *„stdout.txt“* und *„stderr.txt“* in das Speicherkonto hochgeladen wurden. Es empfiehlt sich, umfangreiche Protokolle im Code zum Debuggen auszugeben. |
| 2501         | Can not access user batch account, please check batch account settings (Auf das Batch-Konto des Benutzers kann nicht zugegriffen werden. Überprüfen Sie die Einstellungen des Batch-Kontos.) | Falscher Batch-Zugriffsschlüssel oder Poolname.            | Überprüfen Sie den Poolnamen und den Batch-Zugriffsschlüssel im verknüpften Dienst. |
| 2502         | Can not access user storage account, please check storage account settings. (Auf das Speicherkonto des Benutzers kann nicht zugegriffen werden. Überprüfen Sie die Einstellungen des Speicherkontos.) | Falscher Speicherkontoname oder Zugriffsschlüssel.       | Überprüfen Sie den Speicherkontonamen und den Zugriffsschlüssel im verknüpften Dienst. |
| 2504         | Operation returned an invalid status code 'BadRequest' (Fehler: Der Vorgang hat den ungültigen Statuscode „BadRequest“ zurückgegeben.)     | Zu viele Dateien in „folderPath“ in der benutzerdefinierten Aktivität. (Die Gesamtgröße von „resourceFiles“ darf nicht mehr als 32.768 Zeichen betragen.) | Entfernen Sie nicht benötigte Dateien. Oder zippen Sie sie und fügen Sie einen Befehl zum Entpacken hinzu, um sie zu extrahieren. Verwenden Sie z. B. `powershell.exe -nologo -noprofile   -command "& { Add-Type -A 'System.IO.Compression.FileSystem';   [IO.Compression.ZipFile]::ExtractToDirectory($zipFile, $folder); }" ;  $folder\yourProgram.exe`. |
| 2505         | Eine SAS (Shared Access Signature) kann nur erstellt werden, wenn die Kontoschlüssel-Anmeldeinformationen verwendet werden. | Benutzerdefinierte Aktivitäten unterstützen nur Speicherkonten, die einen Zugriffsschlüssel verwenden. | Weitere Informationen erhalten Sie in der Fehlerbeschreibung.                                            |
| 2507         | The folder path does not exist or is empty: ... (Der Ordnerpfad ist nicht vorhanden oder leer: ...).            | Unter dem angegebenen Pfad sind im Speicherkonto keine Dateien vorhanden.       | „folderPath“ muss die ausführbaren Dateien enthalten, die Sie ausführen möchten. |
| 2508         | There are duplicate files in the resource folder. (Der Ressourcenordner enthält doppelte Dateien.)               | Mehrere Dateien mit dem gleichen Namen sind in verschiedenen Unterordnern von „folderPath“ vorhanden. | Benutzerdefinierte Aktivitäten vereinfachen die Ordnerstruktur unter „folderPath“.  Wenn Sie die Ordnerstruktur beibehalten möchten, zippen Sie die Dateien und extrahieren Sie sie in Azure Batch mit einem Befehl zum Entzippen. Verwenden Sie z. B. `powershell.exe -nologo -noprofile   -command "& { Add-Type -A 'System.IO.Compression.FileSystem';   [IO.Compression.ZipFile]::ExtractToDirectory($zipFile, $folder); }" ;   $folder\yourProgram.exe`. |
| 2509         | Batch url ... is invalid, it must be in Uri format. (Batch-URL ... ist ungültig, die Angabe muss im URI-Format erfolgen.)         | Batch-URLs müssen `https://mybatchaccount.eastus.batch.azure.com` ähneln. | Weitere Informationen erhalten Sie in der Fehlerbeschreibung.                                            |
| 2510         | Fehler beim Senden der Anforderung.               | Die Batch-URL ist ungültig.                                         | Überprüfen Sie die Batch-URL.                                            |

## <a name="hdinsight"></a>HDInsight

Die folgende Tabelle gilt für Spark, Hive, MapReduce, Pig und Hadoop Streaming.

| Fehlercode | Fehlermeldung                                                | BESCHREIBUNG                                                  | Empfehlung                           |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2300, 2310 | Hadoop job submission failed. (Fehler beim Übermitteln des Hadoop-Auftrags.) Fehler Der Remotename konnte nicht aufgelöst werden.“ verursacht. <br/><br/>Der Cluster wurde nicht gefunden. | Der angegebene Cluster-URI ist ungültig.                              | Stellen Sie sicher, dass der Cluster nicht gelöscht wurde und der angegebene URI richtig ist. Wenn Sie die URL in einem Browser öffnen, sollte die Ambari-Benutzeroberfläche angezeigt werden. Wenn sich der Cluster in einem virtuellen Netzwerk befindet, sollte der URI der private URI sein. Um ihn zu öffnen, verwenden Sie eine VM, die Teil desselben virtuellen Netzwerks ist. Weitere Informationen finden Sie unter [„Herstellen einer direkten Verbindung mit Apache Hadoop-Diensten“](https://docs.microsoft.com/azure/hdinsight/hdinsight-extend-hadoop-virtual-network#directly-connect-to-apache-hadoop-services). |
| 2\.300         | Hadoop job submission failed. Job: …, Cluster: …/. (Fehler beim Übermitteln des Hadoop-Auftrags. Auftrag: …, Cluster: …/.) Fehler Eine Aufgabe wurde abgebrochen. | Die Auftragserteilung war zeitlich begrenzt.                         | Das Problem kann entweder ein allgemeines HDInsight-Verbindungs- oder ein Netzwerkverbindungsproblem sein. Bestätigen Sie zunächst, dass die HDInsight Ambari-Benutzeroberfläche von jedem Browser aus verfügbar ist. Bestätigen Sie, dass Ihre Anmeldeinformationen noch gültig sind. Wenn Sie eine selbst gehostete integrierte Laufzeit (IR) verwenden, stellen Sie sicher, dass dies von der VM oder dem Computer aus geschieht, auf denen die selbstgehostete IR installiert ist. Übermitteln Sie dann den Auftrag erneut über Data Factory. Wenn weiterhin Fehler auftreten, wenden Sie sich an das Data Factory-Team. |
| 2\.300         | Nicht autorisiert:   Der Ambari-Benutzername oder das Kennwort ist falsch.  <br/><br/>Nicht autorisiert:   User admin is locked out in Ambari (Nicht autorisiert: Benutzeradministrator ist in Ambari gesperrt.).   <br/><br/>(403) Verboten: Zugriff verweigert.“ | Die Anmeldeinformationen für HDInsight sind falsch oder abgelaufen. | Korrigieren Sie die Anmeldeinformation, und stellen Sie den verknüpften Dienst erneut bereit. Stellen Sie zunächst sicher, dass die Anmeldeinformationen in HDInsight funktionieren. Öffnen Sie dazu den Cluster-URI in einem Browser, und versuchen Sie sich anzumelden. Wenn die Anmeldeinformationen nicht funktionieren, können Sie sie im Azure-Portal zurücksetzen. |
| 2300, 2310 | 502: Webserver hat als Gateway oder Proxyserver eine ungültige Antwort erhalten.       <br/>Ungültiges Gateway. | Dieser Fehler stammt von HDInsight.                               | Dieser Fehler stammt vom HDInsight-Cluster. Weitere Informationen finden Sie unter [„Ambari-Benutzeroberfläche Fehler 502“](https://hdinsight.github.io/ambari/ambari-ui-502-error.html), [„Fehler 502 bei der Verbindung zum Spark Thrift Server“](https://hdinsight.github.io/spark/spark-thriftserver-errors.html), [„Fehler 502 bei der Verbindung zum Spark Thrift Server“](https://hdinsight.github.io/spark/spark-thriftserver-errors.html) und [„Fehlerbehebung bei ungültigen Gateway-Fehlern im Application Gateway“](https://docs.microsoft.com/azure/application-gateway/application-gateway-troubleshooting-502). |
| 2\.300         | Hadoop job submission failed. Job: …, Cluster: ... Error: {\"error\":\"Unable to service the submit job request as templeton service is busy with too many submit job requests. Please wait for some time before retrying the operation. Please refer to the config templeton.parallellism.job.submit to configure concurrent requests.\ (Fehler beim Übermitteln des Hadoop-Auftrags. Auftrag: …, Cluster: ... Fehler: Die Anforderung zur Auftragsübermittlung kann nicht verarbeitet werden, da der Templeton-Dienst mit zu vielen Anforderungen zur Auftragsübermittlung ausgelastet ist. Warten Sie einen Moment, bevor Sie den Vorgang wiederholen. Informationen zum Konfigurieren von gleichzeitigen Anforderungen finden Sie in der Konfiguration „templeton.parallellism.job.submit“.)  <br/><br/>Hadoop job submission failed. Job: 161da5d4-6fa8-4ef4-a240-6b6428c5ae2f, Cluster: `https://abc-analytics-prod-hdi-hd-trax-prod01.azurehdinsight.net/`.   Error: {\"error\":\"java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1561147195099_3730 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.joblauncher already has 500 applications, cannot accept submission of application: application_1561147195099_3730\ (Fehler beim Übermitteln des Hadoop-Auftrags. Auftrag: 161da5d4-6fa8-4ef4-a240-6b6428c5ae2f, 161da5d4-6fa8-4ef4-a240-6b6428c5ae2f, Cluster: ... Fehler: {...java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Fehler beim Übermitteln von application_1561147195099_3730 an YARN: org.apache.hadoop.security.AccessControlException: Warteschlange „root.joblauncher“ enthält bereits 500 Anwendungen, Übermittlung der Anwendung kann nicht angenommen werden: application_1561147195099_3730\) | Zu viele Aufträge werden gleichzeitig an HDInsight übermittelt. | Ziehen Sie in Betracht, die Anzahl der Aufträge, die gleichzeitig an HDInsight übermittelt werden, zu begrenzen. Wenn die Aufträge über die gleiche Aktivität übermittelt werden, finden Sie entsprechende Informationen in der Data Factory-Aktivität für Parallelität. Ändern Sie die Trigger so, dass die gleichzeitigen Pipelineausführungen über die Zeit verteilt werden. Weitere Informationen über die Anpassung von `templeton.parallellism.job.submit`, wie im Fehler angegeben, finden Sie in der HDInsight-Dokumentation. |
| 2303, 2347 | Hadoop job failed with exit code '5'. See 'wasbs://adfjobs@adftrialrun.blob.core.windows.net/StreamingJobs/da4afc6d-7836-444e-bbd5-635fce315997/18_06_2019_05_36_05_050/stderr' for more details. (Fehler beim Hadoop-Auftrag mit Exitcode ‚5‘. Weitere Informationen finden Sie unter ‚...‘.)  <br/><br/>Hive execution failed with error code 'UserErrorHiveOdbcCommandExecutionFailure'.   Weitere Informationen finden Sie unter wasbs://adfjobs@eclsupplychainblobd.blob.core.windows.net/HiveQueryJobs/16439742-edd5-4efe-adf6-9b8ff5770beb/18_06_2019_07_37_50_477/Status/hive.out. | Der Auftrag wurde an HDInsight übermittelt, und in HDInsight ist ein Fehler aufgetreten. | Der Auftrag wurde erfolgreich an HDInsight übermittelt. Bei seiner Ausführung ist ein Fehler im Cluster aufgetreten. Öffnen Sie den Auftrag entweder in der HDInsight Ambari-Benutzeroberfläche, oder öffnen Sie die Datei im Speicher, wie in der Fehlermeldung angegeben. Die Datei zeigt die Fehlerdetails an. |
| 2328         | Internal server error occurred while processing the request. Please retry the request or contact support (Beim Verarbeiten der Anforderung ist ein interner Serverfehler aufgetreten. Wiederholen Sie die Anforderung, oder wenden Sie sich an den Support.) | Dieser Fehler tritt bei HDInsight (bedarfsgesteuert) auf.                              | Dieser Fehler stammt vom HDInsight-Dienst, wenn bei der HDInsight-Bereitstellung Fehler aufgetreten sind. Wenden Sie sich an das HDInsight-Team, und geben Sie den Namen des bedarfsgesteuerten Clusters an. |
| 2310         | java.lang.NullPointerException                               | Dieser Fehler tritt auf, wenn der Auftrag an einen Spark-Cluster gesendet wird.      | Diese Ausnahme kommt von HDInsight. Sie verbirgt das eigentliche Problem. Wenden Sie sich an das HDInsight-Team, um Unterstützung zu erhalten. Informieren Sie das Team über den Clusternamen und die Zeitbereich der Aktivitätsausführung. |
|              | Alle anderen Fehler                                             |                                                              | Weitere Informationen finden Sie unter [Problembehandlung mit HDInsight„](../hdinsight/hdinsight-troubleshoot-guide.md) und [„HDInsight FAQ“](https://hdinsight.github.io/). |



## <a name="web-activity"></a>Webaktivität

| Fehlercode | Fehlermeldung                                                | BESCHREIBUNG                                                  | Empfehlung                          |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2108         | Invalid HttpMethod: '...' (Ungültige HttpMethod: ‚...‘.)                                    | Die Webaktivität unterstützt die HTTP-Methode, die in der Aktivitätsnutzlast angegeben ist, nicht. | Die unterstützten HTTP-Methoden sind „PUT“, „POST“, „GET“ und „DELETE“. |
| 2108         | Invalid Server Error 500 (Fehler: Ungültiger Server 500).                                     | Interner Fehler am Endpunkt.                               | Verwenden Sie Fiddler/Postman, um die Funktionalität für die URL zu überprüfen. |
| 2108         | Unauthorized 401 (401 – Nicht autorisiert).                                             | Fehlende gültige Authentifizierung für die Anforderung.                      | Das Token ist möglicherweise abgelaufen. Stellen Sie eine gültige Authentifizierungsmethode zur Verfügung. Verwenden Sie Fiddler/Postman, um die Funktionalität für die URL zu überprüfen. |
| 2108         | Forbidden 403. (403 – Unzulässig).                                                | Fehlende erforderliche Berechtigungen.                                 | Überprüfen Sie die Benutzerberechtigungen für die Ressource, auf die zugegriffen wird. Verwenden Sie Fiddler/Postman, um die Funktionalität für die URL zu überprüfen.  |
| 2108         | Bad Request 400. (400 – Ungültige Anforderung.)                                              | Ungültige HTTP-Anforderung.                                         | Überprüfen Sie die URL, das Verb und den Text der Anforderung. Verwenden Sie Fiddler oder Postman, um die Anforderung zu überprüfen.  |
| 2108         | Not found 404. (404 – Nicht gefunden.)                                                | Die Ressource wurde nicht gefunden.                                       | Verwenden Sie Fiddler oder Postman, um die Anforderung zu überprüfen.  |
| 2108         | Service unavailable. (Dienst nicht verfügbar.)                                          | Der Dienst ist nicht verfügbar.                                       | Verwenden Sie Fiddler oder Postman, um die Anforderung zu überprüfen.  |
| 2108         | Unsupported Media Type. (Nicht unterstützter Medientyp.)                                       | Der Content-Type stimmt nicht mit dem Text der Webaktivität überein.           | Geben Sie den Inhaltstyp an, der dem Payload-Format entspricht. Verwenden Sie Fiddler oder Postman, um die Anforderung zu überprüfen. |
| 2108         | The resource you are looking for has been removed, has had its name changed, or is temporarily unavailable. (Die gesuchte Ressource wurde entfernt oder umbenannt, oder sie steht vorübergehend nicht zur Verfügung.) | Die Ressource ist nicht verfügbar.                                | Verwenden Sie Fiddler oder Postman, um den Endpunkt zu überprüfen. |
| 2108         | The page you are looking for cannot be displayed because an invalid method (HTTP verb) is being used. (Die gesuchte Seite kann nicht angezeigt werden, da für den Zugriff eine ungültige Methode (HTTP-Verb) verwendet wird.) | Eine falsche Webaktivitätsmethode wurde in der Anforderung angegeben.   | Verwenden Sie Fiddler oder Postman, um den Endpunkt zu überprüfen. |
| 2108         | invalid_payload                                              | The Web Activity body is incorrect. (Falscher Text für die Webaktivität.)                       | Verwenden Sie Fiddler oder Postman, um den Endpunkt zu überprüfen. |

Verwenden Sie Fiddler zum Erstellen einer HTTP-Sitzung der überwachten Webanwendung:

1. Laden Sie [Fiddler](https://www.telerik.com/download/fiddler) herunter, installieren und öffnen sie es.

1. Wenn Ihre Webanwendung HTTPS verwendet, wechseln Sie zu **„Tools“**  >  **„Fiddler-Optionen“**  >  **„HTTPS“** . Wählen Sie **„Capture HTTPS CONNECTs (HTTPS CONNECTs erfassen)“** und **„Decrypt HTTPS Traffic (HTTPS-Datenverkehr entschlüsseln)“** . 
   
   ![Fiddler-Optionen](media/data-factory-troubleshoot-guide/fiddler-options.png)

1. Wenn in der Anwendung SSL-Zertifikate verwendet werden, fügen Sie das Fiddler-Zertifikat auf Ihrem Gerät hinzu. Wechseln Sie zu **„Tools“**  >  **„Fiddler-Optionen“**  >  **„HTTPS“**  >  **„Aktionen“**  >  **„Stammzertifikat auf Desktop exportieren“** .

1. Deaktivieren Sie die Erfassung, indem Sie zu **„Datei“**  >  **„Datenverkehr erfassen“** wechseln. Oder drücken Sie **F12**.

1. Löschen Sie den Browsercache, sodass alle zwischengespeicherten Elemente gelöscht werden und erneut heruntergeladen werden müssen.

1. Erstellen Sie eine Anforderung: 

   a. Wählen Sie die Registerkarte **„Composer“** aus.

   b. Legen Sie die HTTP-Methode und die HTTP-URL fest.

   c. Fügen Sie bei Bedarf Header und den Anforderungstext hinzu.

   d. Wählen Sie **Execute**(Ausführen).

9. Starten Sie die Erfassung des Datenverkehrs wieder, und schließen Sie die entsprechende Transaktion auf Ihrer Seite ab.

10. Navigieren Sie anschließend zu **„Datei“**  >  **„Speichern“**  >  **„Alle Sitzungen“** .

Weitere Informationen finden Sie unter [„Erste Schritte mit Fiddler“](https://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureFiddler)

## <a name="next-steps"></a>Nächste Schritte

Weitere Informationen zur Problembehandlung finden Sie in diesen Ressourcen:

*  [Data Factory-Blog](https://azure.microsoft.com/blog/tag/azure-data-factory/)
*  [Data Factory-Funktionsanfragen](https://feedback.azure.com/forums/270578-data-factory)
*  [Azure-Videos](https://azure.microsoft.com/resources/videos/index/?sort=newest&services=data-factory)
*  [MSDN-Forum](https://social.msdn.microsoft.com/Forums/home?sort=relevancedesc&brandIgnore=True&searchTerm=data+factory)
*  [Stack Overflow-Forum für Data Factory](https://stackoverflow.com/questions/tagged/azure-data-factory)
*  [Twitter-Informationen über Data Factory](https://twitter.com/hashtag/DataFactory)



