---
title: Alle Themen über den Data-Factory-Dienst | Microsoft Docs
description: Tabelle mit allen Themen zum Azure-Dienst namens „Data Factory“ auf „http://azure.microsoft.com/documentation/articles/“, Titel und Beschreibung.
services: data-factory
documentationcenter: ''
author: spelluru
manager: jhubbard
editor: MightyPen

ms.service: data-factory
ms.workload: data-factory
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 10/05/2016
ms.author: spelluru

---
# <a name="all-topics-for-azure-data-factory-service"></a>Alle Themen zum Azure Data Factory-Dienst
In diesem Thema sind alle Themen mit direktem Bezug zum **Data Factory** -Dienst von Azure aufgelistet. Sie können auf dieser Webseite mit **STRG+F**nach Schlüsselwörtern suchen, um aktuell interessante Themen zu finden.

## <a name="new"></a>Neu
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 1 |[Verschieben von Daten mithilfe von Azure Data Factory](data-factory-amazon-redshift-connector.md) |Erfahren Sie, wie Sie Daten aus Amazon Redshift mithilfe von Azure Data Factory verschieben. |
| 2 |[Verschieben von Daten aus Amazon Simple Storage Service mit Azure Data Factory](data-factory-amazon-simple-storage-service-connector.md) |Informationen über das Verschieben von Daten aus Amazon Simple Storage Service (S3) mit Azure Data Factory. |
| 3 |[Azure Data Factory – Kopier-Assistent](data-factory-azure-copy-wizard.md) |Erfahren Sie, wie Sie den Kopier-Assistenten von Azure Data Factory verwenden, um Daten aus unterstützten Datenquellen in Senken zu kopieren. |
| 4 |[Tutorial: Erstellen der ersten Azure Data Factory mit der Data Factory-REST-API](data-factory-build-your-first-pipeline-using-rest-api.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Beispielpipeline mit der Data Factory-REST-API. |
| 5 |[Tutorial: Erstellen einer Pipeline mit Kopieraktivität mithilfe der .NET-API](data-factory-copy-activity-tutorial-using-dotnet-api.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Pipeline mit Kopieraktivität mithilfe der .NET-API. |
| 6 |[Tutorial: Erstellen einer Pipeline mit Kopieraktivität mithilfe der REST-API](data-factory-copy-activity-tutorial-using-rest-api.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Pipeline mit Kopieraktivität mithilfe der REST-API. |
| 7 |[Assistent zum Kopieren in Data Factory](data-factory-copy-wizard.md) |Erfahren Sie, wie Sie den Data Factory-Kopier-Assistenten verwenden, um Daten aus unterstützten Datenquellen in Senken zu kopieren. |
| 8 |[Gateway zur Datenverwaltung](data-factory-data-management-gateway.md) |Richten Sie ein Datengateway ein, um Daten zwischen dem lokalen Speicher und der Cloud zu verschieben. Verwenden Sie das Datenverwaltungsgateway in Azure Data Factory zum Verschieben Ihrer Daten. |
| 9 |[Verschieben von Daten aus einer lokalen Cassandra-Datenbank mit Azure Data Factory](data-factory-onprem-cassandra-connector.md) |Enthält Informationen zum Verschieben von Daten aus einer lokalen Cassandra-Datenbank mit Azure Data Factory. |
| 10 |[Verschieben von Daten aus MongoDB mithilfe von Azure Data Factory](data-factory-on-premises-mongodb-connector.md) |Erfahren Sie, wie Sie Daten aus der MongoDB-Datenbank mithilfe von Azure Data Factory verschieben. |
| 11 |[Verschieben von Daten aus Salesforce mithilfe von Azure Data Factory](data-factory-salesforce-connector.md) |Erfahren Sie, wie Sie Daten mithilfe von Azure Data Factory aus Salesforce verschieben. |

## <a name="updated-articles,-data-factory"></a>Aktualisierte Artikel, Data Factory
In diesem Abschnitt werden Artikel aufgelistet, die kürzlich mit einem sehr umfangreichen oder wesentlichen Update aktualisiert wurden. Für jeden aktualisierten Artikel wird ein grober Ausschnitt des hinzugefügten Markdowntexts angezeigt. Die Artikel wurden zwischen dem **22.08.2016** und dem **05.10.2016** aktualisiert.

| &nbsp; | Artikel | Aktualisierter Text, Ausschnitt | Aktualisierungsdatum |
| ---:|:--- |:--- |:--- |
| 12 |[Azure Data Factory – .NET-API-Änderungsprotokoll](data-factory-api-change-log.md) |Dieser Artikel enthält Informationen zu Änderungen am Azure Data Factory SDK einer bestimmten Version. Das neueste NuGet-Paket für Azure Data Factory finden Sie hier: (https://www.nuget.org/packages/Microsoft.Azure.Management.DataFactories) ** Version 4.11.0** Hinzugefügte Features: / Die folgenden verknüpften Diensttypen wurden hinzugefügt: - OnPremisesMongoDbLinkedService (https://msdn.microsoft.com/library/mt765129.aspx) - AmazonRedshiftLinkedService (https://msdn.microsoft.com/library/mt765121.aspx) - AwsAccessKeyLinkedService (https://msdn.microsoft.com/library/mt765144.aspx) / Die folgenden DataSet-Typen wurden hinzugefügt: - MongoDbCollectionDataset (https://msdn.microsoft.com/library/mt765145.aspx) - AmazonS3Dataset (https://msdn.microsoft.com/library/mt765112.aspx) / Die folgenden Typen von Kopierquellen wurden hinzugefügt: - MongoDbSource (https://msdn.microsoft.com/en-US/library/mt765123.aspx) ** Version 4.10.0** / Die folgenden optionalen Eigenschaften wurden zu TextFormat hinzugefügt:-Ski |2016-09-22 |
| 13 |[Verschieben von Daten in einen und aus einem Azure-Blob mithilfe von Azure Data Factory](data-factory-azure-blob-connector.md) |Definiert das Verhalten beim Kopieren, wenn die Quelle „BlobSource“ oder „FileSystem“ ist.  / **PreserveHierarchy:** Behält die Dateihierarchie im Zielordner bei. Der relative Pfad der Quelldatei zum Quellordner entspricht dem relativen Pfad der Zieldatei zum Zielordner..br/..br/.**FlattenHierarchy**: Alle Dateien aus dem Quellordner befinden sich in der ersten Ebene des Zielordners. Für die Zieldateien wird ein automatisch ein Name erzeugt. .br/..br/.**MergeFiles** (Standardwert): Führt alle Dateien aus dem Quellordner in einer Datei zusammen. Wenn der Datei-/Blob-Name angegeben wurde, entspricht der Name dem angegebenen Namen, andernfalls dem automatisch generierten Dateinamen.  /  No  /  **BlobSource** unterstützt darüber hinaus zum Zweck der Abwärtskompatibilität die beiden folgenden Eigenschaften: / **treatEmptyAsNull**: Gibt an, ob Null oder eine leere Zeichenfolge als NULL-Wert behandelt wird. / **skipHeaderLineCount** : Gibt an, wie viele Zeilen übersprungen werden müssen. Nur anwendbar, wenn das Eingabedataset „TextFormat“ verwendet. Ebenso unterstützt **BlobSink** th |2016-09-28 |
| 14 |[Erstellen von Vorhersagepipelines mithilfe von Azure Machine Learning-Aktivitäten](data-factory-azure-ml-batch-execution-activity.md) |**Webdienst erfordert mehrere Eingaben** Wenn der Webdienst mehrere Eingaben akzeptiert, verwenden Sie die Eigenschaft **webServiceInputs** anstatt **webServiceInput**. Datasets, auf die **webServiceInputs** verweist, müssen auch in der Aktivität **inputs** enthalten sein. In Ihrem Azure ML-Experiment haben Eingabe- und Ausgabeports von Webdiensten und globale Parameter Standardnamen („input1“, „input2“), die Sie anpassen können. Die Namen, die Sie für die Einstellungen webServiceInputs, webServiceOutputs und globalParameters verwenden, müssen den Namen in den Experimenten genau entsprechen. Sie können die Beispiel-Anforderungsnutzlast auf der Hilfeseite für die Batchausführung für Ihren Azure ML-Endpunkt anzeigen, um die erwartete Zuordnung zu überprüfen.    {       "name": "PredictivePipeline",       "properties": {             "description": "use AzureML model",             "activities":  {                "name": "MLActivity",               "type": "AzureMLBatchExecution",                "description": "prediction analysis on batch input",                "inputs":  {                    "name": "inputDataset1"                 }, {                    "name": "inputDatase |2016-09-13 |
| 15 |[Handbuch zur Leistung und Optimierung der Kopieraktivität](data-factory-copy-activity-performance.md) |1. **Einrichten einer Baseline**. Testen Sie Ihre Pipeline mit der Kopieraktivität in der Entwicklungsphase mit repräsentativen Beispieldaten. Mithilfe des Slicing-Modells (data-factory-scheduling-and-execution.md time-series-datasets-and-data-slices) von Data Factory können Sie die verwendete Datenmenge beschränken.  Erfassen Sie mithilfe der **App „Überwachung und Verwaltung“**die Ausführungszeit und die Leistungsmerkmale. Wählen Sie auf Ihrer Data Factory-Startseite die Option **Überwachen und verwalten** aus. Wählen Sie in der Strukturansicht das **Ausgabedataset**aus. Wählen Sie in der Liste **Activity Windows** (Aktivitätsfenster) die ausgeführte Kopieraktivität aus. **Activity Windows** (Aktivitätsfenster) werden die Dauer der Kopieraktivität und die Größe der kopierten Daten aufgeführt. Der Durchsatz ist im **Activity Window Explorer**(Aktivitätsfenster-Explorer) angegeben. Unter Überwachen und Verwalten von Azure Data Factory-Pipelines mit der neuen App „Überwachung und Verwaltung“ erfahren Sie mehr über die App (data-factory-monitor-manage-app.md).  ! Aktivitätsausführung – Details (./media/data-factory-copy-activity-performance/mmapp-activity-run-details.pn |2016-09-27 |
| 16 |[Tutorial: Erstellen einer Pipeline mit Kopieraktivität mithilfe von Visual Studio](data-factory-copy-activity-tutorial-using-visual-studio.md) |Beachten Sie folgende Punkte: - „dataset **type**“ ist auf **AzureBlob** festgelegt.     - **linkedServiceName** ist auf **AzureStorageLinkedService** festgelegt. Sie haben diesen verknüpften Dienst in Schritt 2 erstellt.     - **folderPath** ist auf den Container **adftutorial** festgelegt. Mithilfe der **fileName** -Eigenschaft können Sie auch den Namen eines im Ordner enthaltenen Blobs angeben. Da Sie nicht den Namen des Blobs angeben, werden Daten aus allen Blobs im Container als Eingabedaten betrachtet.    „format **type**“ ist auf **TextFormat** festgelegt. - Die Textdatei enthält die beiden Felder **FirstName** und **LastName**, die durch ein Komma getrennt sind (**columnDelimiter**). - Die Verfügbarkeit (**availability**) ist auf **hourly**, (**frequency** auf **hour** und **interval** auf **1** festgelegt. Der Data Factory-Dienst sucht also stündlich im Stammordner des angegebenen Blobcontainers (**adftutorial**) nach Eingabedaten.  Wenn Sie keinen Dateinamen (**fileName**) für ein **Eingabedataset** angeben, werden alle Dateien/Blobs aus dem Eingabeordner (**folderPath**) als Eingaben betrachtet. |2016-09-29 |
| 17 |[Erstellen, Überwachen und Verwalten von Azure Data Factorys mithilfe des Data Factory .NET SDK](data-factory-create-data-factories-programmatically.md) |Notieren Sie sich die Anwendungs-ID und das Kennwort (geheimer Clientschlüssel), und verwenden Sie beides in der exemplarischen Vorgehensweise. ** Abrufen von Azure-Abonnement- und Mandanten-ID** Wenn Sie nicht die aktuelle Version von PowerShell auf Ihrem Computer installiert haben, befolgen Sie die Anweisungen im Artikel „Installieren und Konfigurieren von Azure PowerShell“ (../powershell-install-configure.md), um sie zu installieren. 1. Starten Sie Azure PowerShell, und führen Sie den folgenden Befehl aus 2. Führen Sie den folgenden Befehl aus, und geben Sie den Benutzernamen und das Kennwort ein, den bzw. das Sie bei der Anmeldung beim Azure-Portal verwendet haben:         Login-AzureRmAccoun Wenn Sie nur ein einziges Azure-Abonnement mit diesem Konto verknüpft haben, müssen Sie die nächsten beiden Schritte nicht ausführen. 3. Führen Sie den folgenden Befehl aus, um alle Abonnements für dieses Konto anzuzeigen:       Get-AzureRmSubscription 4. Führen Sie den folgenden Befehl aus, um das gewünschte Abonnement auszuwählen: Ersetzen Sie **NameOfAzureSubscription** durch den Namen Ihres Azure-Abonnements:       Get-AzureRmSubscription -SubscriptionName NameOfAzureSubscription  /  Set-AzureRmCo |2016-09-14 |
| 18 |[Pipelines und Aktivitäten in Azure Data Factory](data-factory-create-pipelines.md) |,       "start": "2016-07-12T00:00:00Z",    "end": "2016-07-13T00:00:00Z"       }     } Beachten Sie folgende Punkte: / Der Abschnitt „Activities“ enthält nur eine Aktivität, deren **Typ** auf **Copy** festgelegt ist. / Die Eingabe für die Aktivität ist auf **InputDataset** und die Ausgabe für die Aktivität ist auf **OutputDataset** festgelegt. / Im Abschnitt **typeProperties** ist **BlobSource** als Quelltyp und **SqlSink** als Senkentyp angegeben. Eine ausführliche exemplarische Vorgehensweise zum Erstellen dieser Pipeline finden Sie im Tutorial: Kopieren von Daten aus Blob Storage in SQL-Datenbank (data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).. ** Beispielpipeline** In der folgenden Beispielpipeline gibt es im Abschnitt **Aktivitäten** eine Aktivität vom Typ **HDInsightHive**. In diesem Beispiel transformiert die HDInsight Hive-Aktivität (data-factory-hive-activity.md) Daten aus Azure Blob Storage durch Anwenden einer Hive-Skriptdatei auf einen Azure HDInsight Hadoop-Cluster.  {     "name": "TransformPipeline",    "p |2016-09-27 |
| 19 |[Transformieren von Data in Azure Data Factory](data-factory-data-transformation-activities.md) |Data Factory unterstützt die folgenden Transformationsaktivitäten, die Pipelines  (data-factory-create-pipelines.md) entweder einzeln oder mit einer anderen Aktivität verkettet hinzugefügt werden können.  zu erstellen und zu verwalten.  Eine exemplarische Vorgehensweise mit einer detaillierten Anleitung finden Sie im Artikel „Erstellen einer Pipeline mit Hive-Transformation“ (data-factory-build-your-first-pipeline.md). **HDInsight Hive-Aktivität** Die HDInsight Hive-Aktivität in einer Data Factory-Pipeline wendet Hive-Abfragen auf Ihren eigenen oder einen bedarfsgesteuerten Windows-/Linux-basierten HDInsight-Cluster an. Im Artikel „Hive-Aktivität“ (data-factory-hive-activity.md) finden Sie Details zu dieser Aktivität. ** HDInsight Pig-Aktivität ** Die HDInsight Pig-Aktivität in einer Data Factory-Pipeline wendet Pig-Abfragen auf Ihren eigenen oder bedarfsgesteuerten Windows-/Linux-basierten HDInsight-Cluster an. Im Artikel „Pig-Aktivität“ (data-factory-pig-activity.md) finden Sie Details zu dieser Aktivität. ** HDInsight MapReduce-Aktivität** Die HDInsight MapReduce-Aktivität in einer Data Factory-Pipeline wendet MapReduce-Programme auf Ihren |2016-09-26 |
| 20 |[Data Factory – Planung und Ausführung](data-factory-scheduling-and-execution.md) |CopyActivity2 wird nur ausgeführt, wenn CopyActivity1 erfolgreich ausgeführt wurde und Dataset2 verfügbar ist. Hier finden Sie das Beispiel für die Pipeline-JSON:  {       "name": "ChainActivities",    "properties": {           "description": "Run activities in sequence",      "activities":       {       "type": "Copy",     "typeProperties": {     "source": {     "type": "BlobSource"    },      "sink": {       "type": "BlobSink",     "copyBehavior": "PreserveHierarchy",    "writeBatchSize": 0,    "writeBatchTimeout": "00:00:00"     }       },      "inputs":       {       "name": "Dataset1"      }       ,       "outputs":      {       "name": "Dataset2"      }       ,       "policy": {     "timeout": "01:00:00"       },      "scheduler": {      "frequency": "Hour",    "interval": 1       },      "name": "CopyFromBlob1ToBlob2",     "description": "Copy data from a blob to another"       },      {       "type": "Copy",     "typeProperties": {     "source": {     "type": "BlobSource"    },      "sink": {       "type": "BlobSink",     "writeBatchSize": 0,    "writeBatchTimeout": "00:00:00"     }       },      "in |2016-09-28 |

## <a name="tutorials"></a>Tutorials
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 21 |[Tutorial: Erstellen der ersten Pipeline zum Verarbeiten von Daten mithilfe eines Hadoop-Clusters](data-factory-build-your-first-pipeline.md) |In diesem Azure Data Factory-Tutorial erfahren Sie, wie Sie eine Data Factory erstellen und planen, die Daten unter Verwendung eines Hive-Skripts in einem Hadoop-Cluster verarbeitet. |
| 22 |[Tutorial: Erstellen der ersten Azure Data Factory mit einer Azure Resource Manager-Vorlage](data-factory-build-your-first-pipeline-using-arm.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Beispielpipeline mithilfe einer Azure Resource Manager-Vorlage. |
| 23 |[Tutorial: Erstellen der ersten Azure Data Factory mit dem Azure-Portal](data-factory-build-your-first-pipeline-using-editor.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Beispielpipeline mit dem Data Factory-Editor im Azure-Portal. |
| 24 |[Tutorial: Erstellen der ersten Azure Data Factory mit Azure PowerShell](data-factory-build-your-first-pipeline-using-powershell.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Beispielpipeline mithilfe von Azure PowerShell. |
| 25 |[Tutorial: Erstellen der ersten Azure Data Factory mit Microsoft Visual Studio](data-factory-build-your-first-pipeline-using-vs.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Beispielpipeline mithilfe von Visual Studio. |
| 26 |[Tutorial: Erstellen einer Pipeline mit Kopieraktivität mithilfe des Azure-Portals](data-factory-copy-activity-tutorial-using-azure-portal.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Pipeline mit Kopieraktivität mithilfe des Data Factory-Editors im Azure-Portal. |
| 27 |[Tutorial: Erstellen einer Pipeline mit Kopieraktivität mithilfe der Azure PowerShell](data-factory-copy-activity-tutorial-using-powershell.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Pipeline mit Kopieraktivität mithilfe der Azure PowerShell. |
| 28 |[Tutorial: Erstellen einer Pipeline mit Kopieraktivität mithilfe von Visual Studio](data-factory-copy-activity-tutorial-using-visual-studio.md) |In diesem Tutorial erstellen Sie mithilfe von Visual Studio eine Azure Data Factory-Pipeline mit Kopieraktivität. |
| 29 |[Kopieren von Daten aus Blob Storage in SQL-Datenbank mithilfe von Data Factory](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md) |In diesem Tutorial erfahren Sie, wie Sie die Kopieraktivität in einer Azure Data Factory-Pipeline verwenden, um Daten aus Blob Storage in SQL-Datenbank zu kopieren. |
| 30 |[Tutorial: Erstellen einer Pipeline mit Kopieraktivität mithilfe des Data Factory-Kopier-Assistenten](data-factory-copy-data-wizard-tutorial.md) |In diesem Tutorial erstellen Sie eine Azure Data Factory-Pipeline mit einer Kopieraktivität, indem Sie den von der Data Factory unterstützten Kopier-Assistenten verwenden. |

## <a name="data-movement"></a>Datenverschiebung
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 31 |[Verschieben von Daten in einen und aus einem Azure-Blob mithilfe von Azure Data Factory](data-factory-azure-blob-connector.md) |Hier erfahren Sie, wie Sie Blobdaten in Azure Data Factory kopieren. Verwenden Sie unser Beispiel zum Kopieren von Daten aus Azure-Blobspeicher in die Azure SQL-Datenbank (und umgekehrt). |
| 32 |[Verschieben von Daten in und aus Azure Data Lake-Speicher mithilfe von Azure Data Factory](data-factory-azure-datalake-connector.md) |Erfahren Sie, wie Daten mithilfe von Azure Data Factory in und aus Azure Data Lake-Speicher verschoben werden. |
| 33 |[Verschieben von Daten in und aus DocumentDB mithilfe von Azure Data Factory](data-factory-azure-documentdb-connector.md) |Erfahren Sie, wie Daten mithilfe von Azure Data Factory in und aus Azure DocumentDB verschoben werden. |
| 34 |[Verschieben von Daten in und aus Azure SQL-Datenbank mithilfe von Azure Data Factory](data-factory-azure-sql-connector.md) |Erfahren Sie, wie Daten mithilfe von Azure Data Factory in und aus Azure SQL-Datenbank verschoben werden. |
| 35 |[Verschieben von Daten in und aus Azure SQL Data Warehouse mithilfe von Azure Data Factory](data-factory-azure-sql-data-warehouse-connector.md) |Erfahren Sie, wie Daten mithilfe von Azure Data Factory in und aus Azure SQL Data Warehouse verschoben werden. |
| 36 |[Verschieben von Daten in eine und aus einer Azure-Tabelle mithilfe von Azure Data Factory](data-factory-azure-table-connector.md) |Erfahren Sie, wie Daten mithilfe von Azure Data Factory in einen und aus einem Azure-Tabellenspeicher verschoben werden. |
| 37 |[Handbuch zur Leistung und Optimierung der Kopieraktivität](data-factory-copy-activity-performance.md) |Hier erfahren Sie, welche Faktoren sich entscheidend auf die Leistung auswirken, wenn Sie Daten in Azure Data Factory mithilfe der Kopieraktivität verschieben. |
| 38 |[Verschieben von Daten mit der Kopieraktivität](data-factory-data-movement-activities.md) |Informieren Sie sich über das Verschieben von Daten in Data Factory-Pipelines: Datenmigration zwischen Cloudspeichern sowie zwischen lokalen Speichern und Cloudspeichern. Verwenden der Kopieraktivität |
| 11,9 |[Versionshinweise für Datenverwaltungsgateway](data-factory-gateway-release-notes.md) |Versionshinweise für Datenverwaltungsgateway |
| 40 |[Verschieben von Daten aus einem lokalem HDFS mithilfe von Azure Data Factory](data-factory-hdfs-connector.md) |Erfahren Sie, wie Sie Daten aus einem lokalem HDFS mithilfe von Azure Data Factory verschieben. |
| 41 |[Überwachen und Verwalten von Azure Data Factory-Pipelines mit der neuen App „Überwachung und Verwaltung“](data-factory-monitor-manage-app.md) |Informationen zum Verwenden der App „Überwachung und Verwaltung“ für Azure Data Factorys und Pipelines. |
| 42 |[Verschieben von Daten zwischen lokalen Quellen und der Cloud mit dem Datenverwaltungsgateway](data-factory-move-data-between-onprem-and-cloud.md) |Richten Sie ein Datengateway ein, um Daten zwischen dem lokalen Speicher und der Cloud zu verschieben. Verwenden Sie das Datenverwaltungsgateway in Azure Data Factory zum Verschieben Ihrer Daten. |
| 43 |[Verschieben von Daten aus einer OData-Quelle mithilfe von Azure Data Factory](data-factory-odata-connector.md) |Erfahren Sie, wie Sie Daten aus OData-Quellen mithilfe von Azure Data Factory verschieben. |
| 44 |[Verschieben von Daten aus ODBC-Datenspeichern mithilfe von Azure Data Factory](data-factory-odbc-connector.md) |Erfahren Sie, wie Sie Daten aus ODBC-Datenspeichern mithilfe von Azure Data Factory verschieben. |
| 45 |[Verschieben von Daten aus DB2 mithilfe von Azure Data Factory](data-factory-onprem-db2-connector.md) |Erfahren Sie, wie Sie Daten aus der DB2-Datenbank mithilfe von Azure Data Factory verschieben. |
| 46 |[Verschieben von Daten in ein und aus ein lokales Dateisystem mit Azure Data Factory](data-factory-onprem-file-system-connector.md) |Erfahren Sie, wie Daten mit Azure Data Factory in ein und aus ein lokales Dateisystem verschoben werden. |
| 47 |[Verschieben von Daten aus MySQL mithilfe von Azure Data Factory](data-factory-onprem-mysql-connector.md) |Erfahren Sie, wie Sie Daten aus der MySQL-Datenbank mithilfe von Azure Data Factory verschieben. |
| 48 |[Verschieben von Daten in lokales/aus lokalem Oracle mithilfe von Azure Data Factory](data-factory-onprem-oracle-connector.md) |Informationen zum Verschieben von Daten in und aus einer Oracle-Datenbank, die lokal oder mithilfe von Azure Data Factory gehostet wird. |
| 49 |[Verschieben von Daten aus PostgreSQL mithilfe von Azure Data Factory](data-factory-onprem-postgresql-connector.md) |Erfahren Sie, wie Sie Daten aus der PostgreSQL-Datenbank mithilfe von Azure Data Factory verschieben. |
| 50 |[Verschieben von Daten aus Sybase mithilfe von Azure Data Factory](data-factory-onprem-sybase-connector.md) |Erfahren Sie, wie Sie Daten aus der Sybase-Datenbank mithilfe von Azure Data Factory verschieben. |
| 51 |[Verschieben von Daten aus Teradate mithilfe von Azure Data Factory](data-factory-onprem-teradata-connector.md) |Informationen zum Teradata-Connector für den Data Factory-Dienst, mit dem Sie Daten aus einer Teradata-Datenbank verschieben können |
| 52 |[Verschieben von Daten in und aus SQL Server in einer lokalen oder IaaS-Umgebung (Azure-VM) mithilfe von Azure Data Factory](data-factory-sqlserver-connector.md) |Informationen zum Verschieben von Daten in und aus einer SQL Server-Datenbank, die lokal oder mithilfe von Azure Data Factory in einer Azure-VM gehostet wird. |
| 53 |[Verschieben von Daten aus einer Webtabelle mithilfe von Azure Data Factory](data-factory-web-table-connector.md) |Erfahren Sie, wie Sie Daten aus einer lokalen Tabelle mithilfe von Azure Data Factory auf eine Webseite verschieben. |

## <a name="data-transformation"></a>Datentransformation
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 54 |[Erstellen von Vorhersagepipelines mithilfe von Azure Machine Learning-Aktivitäten](data-factory-azure-ml-batch-execution-activity.md) |Beschreibt das Erstellen von Vorhersagepipelines mithilfe von Azure Data Factory und Azure Machine Learning |
| 55 |[Verknüpfte Computedienste](data-factory-compute-linked-services.md) |Erfahren Sie mehr über Compute-Umgebungen, die in Azure Data Factory-Pipelines für die Transformation und Verarbeitung von Daten verwendet werden können. |
| 56 |[Verarbeiten umfangreicher Datasets mit Data Factory und Batch](data-factory-data-processing-using-batch.md) |Beschreibt, wie Sie große Datenmengen in einer Azure Data Factory-Pipeline verarbeiten, indem Sie die parallele Verarbeitung von Azure Batch nutzen. |
| 57 |[Transformieren von Data in Azure Data Factory](data-factory-data-transformation-activities.md) |Informationen Sie zum Transformieren von Daten oder Verarbeiten von Daten in Azure Data Factory mit Hadoop, Machine Learning und Azure Data Lake Analytics. |
| 58 |[Hadoop-Streamingaktivität](data-factory-hadoop-streaming-activity.md) |Erfahren Sie, wie Sie die Hadoop-Streamingaktivität in Azure Data Factory verwenden können, um Hadoop-Streamingprogramme in einem bedarfsgesteuerten oder einem eigenen HDInsight-Cluster auszuführen. |
| 59 |[Hive-Aktivität](data-factory-hive-activity.md) |Erfahren Sie, wie Sie die Hive-Aktivität in Azure Data Factory verwenden können, um Hive-Abfragen in einem bedarfsgesteuerten/eigenen HDInsight-Cluster auszuführen. |
| 60 |[Aufrufen von MapReduce-Programmen über Data Factory](data-factory-map-reduce.md) |Erfahren Sie, wie Sie Daten verarbeiten, indem Sie MapReduce-Programme mithilfe einer Azure Data Factory auf einen Azure HDInsight-Cluster anwenden. |
| 61 |[Pig-Aktivität](data-factory-pig-activity.md) |Erfahren Sie, wie Sie die Pig-Aktivität in Azure Data Factory verwenden können, um Pig-Abfragen in einem bedarfsgesteuerten/eigenen HDInsight-Cluster auszuführen. |
| 62 |[Aufrufen von Spark-Programmen aus Data Factory](data-factory-spark.md) |Erfahren Sie, wie Sie Spark-Programme mithilfe der MapReduce-Aktivität aus einer Azure Data Factory aufrufen. |
| 63 |[SQL Server-Aktivität "Gespeicherte Prozedur"](data-factory-stored-proc-activity.md) |Informationen, wie Sie die SQL Server-Aktivität "Gespeicherte Prozedur" in einer Data Factory-Pipeline zum Aufrufen einer gespeicherten Prozedur in einer Azure SQL-Datenbank oder einem Azure SQL Data Warehouse verwenden können. |
| 64 |[Verwenden von benutzerdefinierten Aktivitäten in einer Azure Data Factory-Pipeline](data-factory-use-custom-activities.md) |Erfahren Sie, wie Sie benutzerdefinierte Aktivitäten erstellen und in einer Azure Data Factory-Pipeline verwenden. |
| 65 |[Ausführen eines U-SQL-Skripts auf Azure Data Lake Analytics in Azure Data Factory](data-factory-usql-activity.md) |Erläutert die Datenverarbeitung durch Ausführen eines U-SQL-Skripts mit einem Azure Data Lake Analytics-Computedienst. |

## <a name="samples"></a>Beispiele
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 66 |[Azure Data Factory Editor – Beispiele](data-factory-samples.md) |Bietet Informationen zu Beispielen, die zum Funktionsumfang des Azure Data Factory-Diensts gehören. |

## <a name="use-cases"></a>Anwendungsfälle
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 67 |[Fallstudien](data-factory-customer-case-studies.md) |Erfahren Sie, wie einige unserer Kunden Azure Data Factory verwendet haben. |
| 68 |[Anwendungsfall – Erstellen von Kundenprofilen](data-factory-customer-profiling-usecase.md) |Erfahren Sie, wie Azure Data Factory zum Erstellen eines datengesteuerten Workflows (Pipeline) verwendet wird, um Profile von Spielekunden zu erstellen. |
| 69 |[Anwendungsfall – Produktempfehlungen](data-factory-product-reco-usecase.md) |Informationen zu einem Anwendungsfall, der mithilfe von Azure Data Factory zusammen mit anderen Diensten implementiert wurde. |

## <a name="monitor-and-manage"></a>Überwachen und Verwalten
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 70 |[Überwachen und Verwalten von Azure Data Factory-Pipelines](data-factory-monitor-manage-pipelines.md) |Erfahren Sie, wie Sie von Ihnen erstellte Azure Data Factorys und Pipelines mithilfe des Azure-Portals und Azure PowerShell überwachen und verwalten. |

## <a name="sdk"></a>SDK
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 71 |[Azure Data Factory – .NET-API-Änderungsprotokoll](data-factory-api-change-log.md) |Beschreibt Änderungen, hinzugefügte Features und Fehlerbehebungen usw. in einer bestimmten Version der .NET-API für Azure Data Factory. |
| 72 |[Erstellen, Überwachen und Verwalten von Azure Data Factorys mithilfe des Data Factory .NET SDK](data-factory-create-data-factories-programmatically.md) |Erfahren Sie, wie Sie Azure Data Factorys mithilfe des Data Factory .NET SDK programmgesteuert erstellen, überwachen und verwalten. |
| 73 |[Azure Data Factory-Entwicklerreferenz](data-factory-sdks.md) |Lernen Sie die verschiedenen Möglichkeiten zum Erstellen, Überwachen und Verwalten von Azure Data Factorys kennen. |

## <a name="miscellaneous"></a>Verschiedenes
| &nbsp; | Titel | Beschreibung |
| ---:|:--- |:--- |
| 74 |[Azure Data Factory – Häufig gestellte Fragen](data-factory-faq.md) |Häufig gestellte Fragen zu Azure Data Factory |
| 75 |[Azure Data Factory – Funktionen und Systemvariablen](data-factory-functions-variables.md) |Enthält eine Liste der Funktionen und Systemvariablen von Azure Data Factory. |
| 76 |[Azure Data Factory – Benennungsregeln](data-factory-naming-rules.md) |Beschreibt die Benennungsregeln für Data Factory-Entitäten. |
| 77 |[Problembehandlung bei Data Factory](data-factory-troubleshoot.md) |Erfahren Sie, wie Sie Probleme mithilfe von Azure Data Factory beheben. |

<!--HONumber=Oct16_HO2-->


